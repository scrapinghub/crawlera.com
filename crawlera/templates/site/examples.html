{% extends "page.html" %}

{% block nav-examples %} class="active"{% endblock %}

{% block page-content %}
<div class="row">
  <div class="span10">
<h3>Examples</h3>

<h4>cURL</h4>
<p>Crawlera provides a standard HTTP proxy interface so you can use it with any software that supports them.
Here is an example using Crawlera with curl to download <a href="http://crawlera.com">http://crawlera.com</a>:

<pre>
  curl -x proxy.crawlera.com:8010 -U USER:PASS http://crawlera.com
</pre>


<h4>Command-line tools</h4>
<p>Several Unix commands (like wget and curl) and applications (including Scrapy) support the http_proxy environment variable to configure the HTTP proxy to use.
You can configure before running your command with:
<pre>
  export http_proxy=http://USER:PASS@proxy.crawlera.com:8010
</pre>

<h4>API</h4>
<p>You can use REST API to fetch content.
<p>Currently this is the only way to fetch sites via HTTPS
<pre>
  curl http://USER:PASS@proxy.crawlera.com:8010/fetch?url=http://crawlera.com
</pre>

<h4>Scrapy</h4>
<p>To use Crawlera with Scrapy you could just set the http_proxy
    environment setting (as explained in the previous
section). However, if you need more functionality (like
configuring which spiders to send through the proxy) you can use
the HubProxyMiddleware, provided in the scrapylib project.

<p>Download the scrapylib project, and enable the middleware by
adding this to your Scrapy settings:
<pre>
  DOWNLOADER_MIDDLEWARES = {'scrapylib.hubproxy.HubProxyMiddleware': 600}
</pre>
</div>
{% endblock %}
