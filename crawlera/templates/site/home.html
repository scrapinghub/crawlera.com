{% extends "page.html" %}

{% block page-content %}

<div class="row">
  <div class="span12">
    <h3 class="">What is Crawlera?</h3>
  </div>
</div>

<div class="row">
  <div class="span10">
    <p align="justify" class="">Crawlera is a smart HTTP/HTTPS downloader designed specifically for
    web crawling and scraping. It routes requests through a pool of IPs,
    throttling access by introducing delays and discarding IPs from the pool
    when they get banned from certain domains, or have other problems.

    As a scraping user, you no longer have to worry about tinkering with
    download delays, concurrent requests, user agents, cookies or referrers to
    avoid getting banned, you just use Crawlera to download pages instead.

    Some plans provide a standard HTTP proxy API, so you can configure it in
    your crawler of choice and start crawling.
  </div>
</div>
<div class="row">
  <div class="span10">
    <h3 class="">How does it work?</h3>
    <p align="justify" class="">Crawlera distributes requests among many internal nodes, using a
proprietary algorithm to minimize the risks of getting banned, by throttling
requests sent to sites from each internal node. If, for whatever reason, any
node gets banned, Crawlera will blacklist it and avoid using it for future
requests to that domain.
</p>

<p align="justify" class="">Banned requests typically return a non-200 response (like 403 or 503), or
redirect to a captcha page. These responses are detected by Crawlera and the
requests are automatically retried from another (clean) node. This is just a
very brief introduction of what happens inside Crawlera, the details are more
sophisticated.</p>

<p align="justify" class="">All this logic happens inside Crawlera and the user doesn't have to worry
about it. The user receives a clean response or a 503 error if all internal
nodes are blacklisted (which usually doesn't happen, except for very popular
domains). Only successful downloads are charged.</p>

<p align="justify" class="">If you need more bandwidth, you can contact support and reserve exclusive
nodes to increase the capacity, or supply your own nodes (as HTTP proxies) and
use them from Crawlers to benefit from its smart downloading mechanism.

See <a href="{%url pricing %}">Pricing</a> for more details.
</p>

    <h3 class="">Example</h3>
    <p align="justify" class="">Using Crawlera can be as easy as running following command:
    <pre class="">  curl -x proxy.crawlera.com:8010 -U USER:PASS http://crawlera.com</pre>
    <div align="center" class=""><a class="btnRedirect" href="http://scrapinghub.com/crawlera"><input type="submit" value="Learn More "></a></div>
  </div>
</div>
{% endblock %}
