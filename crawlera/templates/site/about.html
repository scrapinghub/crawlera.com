{% extends "page.html" %}

{% block body_class %}about{% endblock %}

{% block nav-about %} class="active"{% endblock %}
 
{% block page-title %}About Us{% endblock %}
 
{% block page-content %}
<div class="row">
  <div class="large-10 columns">
    <h2>Overview</h2>
    <p>Crawlera provides an HTTP proxy, with a pool of rotating IPs, designed specifically for scraping purposes. Although it provides a standard HTTP proxy interface it does a lot more internally, like throttling access to domains by introducing delays and discarding IPs from the pool when they get banned or have other problems. As a scraping user, you no longer have to worry about tinkering with download delays, concurrent requests, user agents, cookies or referrers to avoid getting banned from sites, you just configure the proxy and fire up your crawler of choice and start scraping.
    
    <h2>How does it work?</h2>
    <p>Crawlera architecture is based on a group of “master” proxies that receive the user requests and distribute them among many internal “slave” proxies.
    <p>The internal slave proxies are just plain simple HTTP proxies (like one you would run with Squid or similar software) with no extra logic.
    The masters, however, implement a proprietary algorithm to minimize the risks of getting banned, by throttling requests sent to sites from each internal slave, among other techniques. If, for whatever reason, any slaves do get banned anyway, the masters will detect and avoid using them in the future for those particular sites.
    <p>Banned requests typically return a non-200 response (usually
    503 or 403) or will redirect to a captcha page. These responses
    are detected by Crawlera masters and the requests are
    automatically retried from other (clean) slaves. Banned slaves are
    blacklisted to prevent using them again for that domain. All this
    logic happens inside Crawlera infrastructure and the user never
    receives the banned response, nor is charged for them (only
    successful requests are charged). The user may get a 503 response
    from Crawlera if there are no more clean slaves left to try for a
    particular domain. In this case, the user may choose to reserve
    dedicated slaves to increase the capacity. Also, thanks to how
    Crawlera architecture works, users can supply their own proxies to
    be used as slave. See Pricing for more details.

<h2>About us</h2>
<div class="person">
  <p><b>Pablo Hoffman</b> - Founder</p>
  <p>Pablo has been working on a wide range of
  web scraping projects since 2007. He's the lead developer of <a class="external"
  href="http://scrapy.org">Scrapy</a>, an open source screen scraping framework
  for Python. He also founded <a class="external" href="http://insophia.com">Insophia</a>, a
  company that provides screen scraping and system administration help to
  bootstrap your startup.</p>
</div>
 
<div class="person">
  <p><b>Shane Evans</b> - Founder</p>
  <p>Shane has been involved in startups since joining
  lastminute.com in 1999, where he led development of the most critical projects
  there for many years. Since that time he has been part of the founding team for
  a few companies, always seeking out interesting technical challenges. He was
  introduced to web crawling when leading development of the search systems at
  mydeco.com. Some of the web crawling code he wrote at that time was open
  sourced as the <a class="external" href="http://scrapy.org">Scrapy</a> project.</p>
</div>

</div>
</div class="large-4 columns">

</div>
</div>

{% endblock %}




